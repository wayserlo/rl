{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокая Q-сеть, вариант многослойного перцептрона (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания: 16 апреля, 23:59 GMT+3\n",
    "\n",
    "В данной домашней работе требуется реализовать DQN &mdash; приближённый алгоритм Q-обучения c механизмом _experience_ _replay_ и целевой сетью (_target network_) &mdash; и пронаблюдать, лучше ли подход DQN работает по сравнению с рассмотренными ранее в каком-либо аспекте.\n",
    "\n",
    "Исходная статья:\n",
    "https://arxiv.org/pdf/1312.5602.pdf\n",
    "\n",
    "Конспект теории по глубокому Q-обучению:\n",
    "https://arxiv.org/pdf/2201.09746.pdf (главы 4.1 и 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: ФИО, номер группы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании требуется обучить агента в среде CartPole. По идее, процесс обучения должен занять несколько десятков минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a47eb41ef8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import utils\n",
    "from IPython.display import HTML\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole, снова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # CartPole обёрнут в time limit wrapper по умолчанию\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приближённое Q-обучение: построение сети\n",
    "\n",
    "Теперь требуется построить нейронную сеть, способную отображать наблюдения в Q-ценности состояния. Так как происходит работа с предобработанными признаками (положения тележки, углы и скорости), то не требуется сложная нейронная сеть пока. Фактически, нужно построить нейронную сеть с архитектурой, обозначенной снизу:\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png width=640>\n",
    "\n",
    "В идеале рекомендуется начать с небольших 1-2 скрытых слоёв с < 200 нейронов и затем увеличить размер сети, если агент не может получить целевую награду. Батч-нормализация и дропаут здесь могут всё испортить. Также рекомендуется избегать использование таких нелинейностей, как функция сигмоиды и гиперболического тангенса: наблюдения агента не нормализованы, так что сигмоиды могут концентрироваться у нуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# те, кто имеют GPU, но не хотят использовать графический вычислитель, могут раскомментировать:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Задайте структуру нейронной сети здесь. \n",
    "        # Убедитесь в том, что здесь определёны все параметры агента.\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        < Ваш код >\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        принимает наблюдение агента (тензор), возвращает qvalues (тензор)\n",
    "        :параметр state_t: батч состояний, размерность = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Используйте Вашу нейросеть для вычисления qvalues данного состояния state_t\n",
    "        qvalues = < Ваш код >\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues должны быть torch tensor с возможностью дифференцирования\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        Как forward, только работает на numpy arrays, не на тензорах\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"Выбрать действия для данных qvalues. Использует эпсилон-жадную стратегию исследования среды.\"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте проверим нашего агента на предмет наличия ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Играет qvalues полных эпизодов. Если greedy=True, то действия берутся из детерминированной\n",
    "        стратегии как argmax(qvalues). Возвращает среднюю награду. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    env.close()\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, n_games=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "Сущетвует мощный метод, который можно использовать для улучшения сходимости _off-policy_ алгоритмов относительно общего количества используемых сэмплов: _Experience_ _Replay_. Смысл состоит в том, что появляется возможность настроить агента с помощью Q-обучения и EV-SARSA на кортежах `<s,a,r,s'>`, даже если они не сэмплированы из текущей политики агента. Итак, здесь требуется реализовать следующее:\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=640>\n",
    "\n",
    "#### Обучение с experience replay\n",
    "1. Сыграть шаг игры, отсэмплировать `<s,a,r,s'>`.\n",
    "2. Обновить q-values на основе `<s,a,r,s'>`.\n",
    "3. Записать `<s,a,r,s'>` в буфер. \n",
    " 3. Если буфер полный, то удалить ранние данные.\n",
    "4. Сэмплировать K таких переходов из данного буфера и обновить q-values на их основе.\n",
    "\n",
    "#### Интерфейс довольно простой:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)`  &mdash; сохраняет кортеж (s,a,r,s',done) в буфер\n",
    "* `exp_replay.sample(batch_size)` &mdash; возвращает наблюдения, действия, награды, следующие наблюдения и is_done для `batch_size` случайных сэмплов.\n",
    "* `len(exp_replay)` &mdash; возвращает количество элиментов, хранящихся в буфере на данный момент.\n",
    "\n",
    "\n",
    "Для применения такого обучения требуется, во-первых, реализация структуры памяти, которая будет работать как буфер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Создаёт буфер реплеев.\n",
    "        Параметры\n",
    "        ----------\n",
    "        size: int\n",
    "            Максимальное количество хранящихся единовременно переходов. При переполнении буфера старые\n",
    "            записи удаляются.\n",
    "\n",
    "        Замечание: для данного задания можно выбрать любую структуру данных.\n",
    "              Если Вам достаточно простого решения, то можно просто хранить список кортежей (s, a, r, s')\n",
    "              в self._storage. Однако, можно найти более быстрые и/или эффективные по памяти способы\n",
    "              реализации самого хранения переходов.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "\n",
    "        # Дополнительно: Ваш код\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Убедитесь, что _storage не превзойдёт по размерам _maxsize. \n",
    "        Убедитесь, что FIFO правило выполняется: старейшие прецеденты должны удаляться раньше всех.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        storage = self._storage\n",
    "        maxsize = self._maxsize\n",
    "        < Ваш код >\n",
    "            # добавить данные в storage\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Сэмплирование батча переходов.\n",
    "        Параметры\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Сколько переходов сэмплировать.\n",
    "        Возвращает\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            батч наблюдений (состояний)\n",
    "        act_batch: np.array\n",
    "            батч действий, выполненных на основе obs_batch\n",
    "        rew_batch: np.array\n",
    "            награды, полученные в качестве результата выполнения act_batch\n",
    "        next_obs_batch: np.array\n",
    "            следующие наблюдения (состояния), полученные после выполнения act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1, если выполнение act_batch[i] повлекло\n",
    "            окончание эпизода и 0 иначе.\n",
    "        \"\"\"\n",
    "        storage = self._storage\n",
    "        < Ваш код >\n",
    "            # < случайно сгенерировать batch_size индексов сэмплов в буфере >\n",
    "            \n",
    "        < Ваш код >\n",
    "            # собрать <s,a,r,s',done> для каждого индекса\n",
    "        \n",
    "        return < Ваш код >\n",
    "            # < states > , < actions >, < rewards >,  < next_states >, < is_done >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
    "                   1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"размер буфера должен быть равен 10, потому что это максимальная вместимость\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Сыграть в игру n_steps шагов, записать каждый (s,a,r,s', done) в exp_replay буфер.\n",
    "    Как только игра заканчивается, добавляется запись с done=True и среда env перезапускается.\n",
    "    Гарантируетcя, что env находится в состоянии done=False после работы функции.\n",
    "\n",
    "    ПРОСЬБА НЕ ПЕРЕЗАПУСКАТЬ ENV, ПОКА ОНА НЕ В СОСТОЯНИИ \"DONE\"\n",
    "\n",
    "    :возвращает: сумму наград по времени и состояние, в котором среда оказалась\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Сыграть в игру n_steps шагов согласно инструкциям выше\n",
    "    < Ваш код >\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируйте Ваш код.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# если вы используете собственный experience replay буфер, то некоторые из тестов, возможно, потребуется\n",
    "# скорректировать.\n",
    "# Убедитесь в том, что Вы знаете, что Ваш код делает\n",
    "assert len(exp_replay) == 1000, \"play_and_record должна добавить ровно 1000 шагов, \"\\\n",
    "                                 \"но вместо этого добавила %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1,\\\n",
    "\"Убедитесь, что Вы перезапускаете игру, как только она 'done', и корректно записываете is_done в буфер.\"\\\n",
    "\"Получена %f доля is_done за %i шагов. [Если думаете, что Вам не повезло, то просто перезапустите тест.]\" % (\n",
    "                                        np.mean(is_dones), len(exp_replay))\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,), \"батч действий должен иметь размер (10,), но имееет %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"батч наград должен иметь размер (10,), но имееет %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"батч is_done должен иметь размер (10,), но имееет %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done должен быть строго True или False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch], \"действия должны быть в промежутке [0, n_actions]\"\n",
    "\n",
    "print(\"Отлично справились!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Целевые сети\n",
    "\n",
    "Также будет реализована так называемая \"целевая сеть\" &mdash; копия весов нейронной сети агента, используемая для оценки целевого значения Q-функции:\n",
    "\n",
    "Сама по себе сеть представляет собой точную копию сети агента, но её параметры не обучаются. Наоборот, они берутся из обучаемой сети агента раз в несколько итераций цикла обучения.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png width=480>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# Так можно загружать веса из агента в целевую сеть\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с помощью... Q-обучения\n",
    "Тут будет реализована функция, похожая на `agent.update` из табличного Q-обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислить TD ошибку Q-обучения:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "С $Q_{reference}$, определённой как:\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Где\n",
    "* $Q_{target}(s',a')$ обозначает Q-ценность следующего состояния и следующего действия, предсказанного с помощью __target_network__\n",
    "* $s, a, r, s'$ суть текущие состояние, действие, награда и следующее состояние соответственно\n",
    "* $\\gamma$ фактор дисконта, определённый двумя блоками выше.\n",
    "\n",
    "\n",
    "__Замечание 1:__ ниже представлен пример входных данных. Не бойтесь с ним экспериментировать, прежде чем написать непосредственно функцию.\n",
    "\n",
    "__Замечание 2:__ compute_td_loss есть источник приблизительно 99% ошибок в данной домашней работе. Если награда не увеличивается, то часто в подобных случаях помогает построчная отладка с помощью [резиновой утки](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Вычислите td функцию потерь, используя только операции PyTorch. Используйте формулы выше. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # размер: [batch_size, *state_shape]\n",
    "    # print('S', states)\n",
    "\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # размер: [batch_size]\n",
    "    # print('A', actions)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # размер: [batch_size]\n",
    "    # print('R', rewards)\n",
    "    # размер: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    # print(\"S'\", next_states)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # размер: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "    # print('done', is_not_done)\n",
    "\n",
    "    # получить q-values для всех действий в текущих состояниях\n",
    "    predicted_qvalues = agent(states)\n",
    "    # print('Q(s,a)', predicted_qvalues)\n",
    "\n",
    "    # вычислить q-values для всех действий в следующих состояниях\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    # print(\"Q(s',a')\", predicted_next_qvalues)\n",
    "    \n",
    "    # выбрать q-values для соотоветствующих выбранных действий\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "    # print('selected Q(s, a)', predicted_qvalues_for_actions)\n",
    "\n",
    "    # вычислить V*(next_states), используя предсказанные следующие q-values\n",
    "    next_state_values = < Ваш код >\n",
    "\n",
    "    assert next_state_values.dim(\n",
    "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"нужно предсказывать одно действие для состояния\"\n",
    "\n",
    "    # вычислить \"target q-values\" для функции потерь - то, что внутри квадратных скобок в формуле выше.\n",
    "    # в последнем состоянии использовать упрощённую формулу: Q(s,a) = r(s,a) так как s' не существует\n",
    "    # Вы можете умножить следующие состояния на is_not_done для достижения этого.\n",
    "    target_qvalues_for_actions = < Ваш код >\n",
    "\n",
    "    # среднеквадратическая функция потерь для минимизации\n",
    "    \n",
    "    assert predicted_qvalues_for_actions.shape == target_qvalues_for_actions.shape\n",
    "    \n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "    # print('loss vector', (predicted_qvalues_for_actions -\n",
    "    #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "    # print('loss', loss)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что Вы предсказали все q-values для всех действий в следующих состояниях\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что Вы вычислили V(s') как максимум по оси действий в тензоре, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не то с целевыми q-values, они должны быть вектором\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка корректности реализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
    "    ), \"Вы должны вернуть скаляр значения функции потерь - среднее по батчу.\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
    "              0), \"Функция потерь должна быть дифференцируема относительно весов сети.\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \"Целевая сеть не должна быть дифференцируемой.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основной цикл\n",
    "\n",
    "Время собрать всё вместе и посмотреть, обучается ли что-нибудь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = < На ваше усмотрение >\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10**4)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Меньше, чем 100 Mb RAM, доступно. \n",
    "            Убедитесь, что размер буфера не слишком большой.\n",
    "            Также проверьте, может, другие процессы потребляют сильно много RAM.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 100\n",
    "batch_size = 42\n",
    "total_steps = 3 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.RMSprop(agent.parameters(), lr=1e-3)\n",
    "opt.zero_grad()\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 10\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for step in trange(total_steps + 1):\n",
    "    if not utils.is_enough_ram():\n",
    "        print('Меньше, чем 100 Mb RAM, доступно, заморозка процедуры обучения')\n",
    "        print('убедитесь, что всё в порядке и пошлите сигнал KeyboardInterrupt для продолжения')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "    # игра\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "    # обучение\n",
    "    < сэмплировать batch_size переходов из experience replay >\n",
    "\n",
    "    loss = < вычислить TD функцию потерь >\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm).item()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Загрузка весов агента в target_network\n",
    "        < Ваш код >\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        # Оценка агента\n",
    "        mean_rw_history.append(evaluate(\n",
    "            make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "        )\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [make_env(seed=step).reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        print(\"размер буфера = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Средняя награда за эпизод\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"История TD функции потерь (сглаженная)\")\n",
    "        plt.plot(utils.smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Ценность начального состояния\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"История нормы градиента (сглаженная)\")\n",
    "        plt.plot(utils.smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('финальный счёт:', final_score)\n",
    "assert final_score > 300, 'недостаточно хорошо для DQN'\n",
    "print('Отлично справились')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись эпизодов\n",
    "env_monitor = gym.wrappers.Monitor(env, directory=\"CartPole-v1videos\", force=True)\n",
    "evaluate(env_monitor, agent)\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./CartPole-v1videos\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./CartPole-v1videos/\" + video_names[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"CartPole-v1videos-deterministic\", force=True)\n",
    "evaluate(env_monitor, agent, greedy=True)\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./CartPole-v1videos-deterministic\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./CartPole-v1videos-deterministic/\" + video_names[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
